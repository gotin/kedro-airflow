import os
import json

from typing import Dict
from pathlib import Path

from airflow import DAG
from airflow.models import BaseOperator
from airflow.utils.decorators import apply_defaults
from airflow.version import version
from datetime import datetime, timedelta

from kedro.framework.session import KedroSession
from kedro.framework.project import configure_project
from airflow.providers.databricks.hooks.databricks import DatabricksHook


class KedroOperator(BaseOperator):

    @apply_defaults
    def __init__(
        self,
        package_name: str,
        pipeline_name: str,
        node_name: str,
        project_path: str,
        env: str,
        *args, **kwargs
    ) -> None:
        super().__init__(*args, **kwargs)
        self.package_name = package_name
        self.pipeline_name = pipeline_name
        self.node_name = node_name
        self.project_path = project_path
        self.env = env


    def _parse_host(host: str) -> str:
        """
        The purpose of this function is to be robust to improper connections
        settings provided by users, specifically in the host field.
        For example -- when users supply ``https://xx.cloud.databricks.com`` as the
        host, we must strip out the protocol to get the host.::
            h = DatabricksHook()
            assert h._parse_host('https://xx.cloud.databricks.com') == \
                'xx.cloud.databricks.com'
        In the case where users supply the correct ``xx.cloud.databricks.com`` as the
        host, this function is a no-op.::
            assert h._parse_host('xx.cloud.databricks.com') == 'xx.cloud.databricks.com'
        """
        urlparse_host = urlparse(host).hostname
        if urlparse_host:
            # In this case, host = https://xx.cloud.databricks.com
            return urlparse_host
        else:
            # In this case, host = xx.cloud.databricks.com
            return host


    def _save_db_connect_conf(self, databricks_conn_id: str):
        hook = DatabricksHook(databricks_conn_id = databricks_conn_id)
        if 'host' in hook.databricks_conn.extra_dejson:
            host = self._parse_host(self.databricks_conn.extra_dejson['host'])
        else:
            host = hook.databricks_conn.host
        databricks_conn_config = dict(
            host=host + "dummy",
            token=hook.databricks_conn.extra_dejson['token'],
            cluster_id=hook.databricks_conn.extra_dejson['cluster_id'],
            org_id=hook.databricks_conn.extra_dejson.get('org_id', '0'),
            posrt=hook.databricks_conn.extra_dejson.get('port', '15001'),
        )
        home = os.environ["HOME"]
        with open(Path.joinpath(Path(home), ".databricks-connect"), "w") as f:
            f.write(json.dumps(databricks_conn_config))


    def _prepare_db_connect_conf(self, databricks_conn_id: str) -> Dict[str, str]:
        hook = DatabricksHook(databricks_conn_id = databricks_conn_id)
        if 'host' in hook.databricks_conn.extra_dejson:
            host = self._parse_host(self.databricks_conn.extra_dejson['host'])
        else:
            host = hook.databricks_conn.host
        return {
            "spark.databricks.service.client.enabled" : "true",
            "spark.databricks.service.address" : host,
            "spark.databricks.service.token" : hook.databricks_conn.extra_dejson['token'],
            "spark.databricks.service.clusterId" : hook.databricks_conn.extra_dejson['cluster_id'],
            "spark.databricks.service.orgId" : hook.databricks_conn.extra_dejson.get('org_id', '0'),
            "spark.databricks.service.port": hook.databricks_conn.extra_dejson.get('port', '15001'),
        }


    def execute(self, context):
        configure_project(self.package_name)
        with KedroSession.create(self.package_name,
                                 self.project_path,
                                 env=self.env) as session:
            databricks_conn_id = session.load_context().params.get("databricks_connection_id", "databricks_default") # 
            self._save_db_connect_conf(databricks_conn_id) #
            db_connect_conf = self._prepare_db_connect_conf(databricks_conn_id) #
            session.load_context().init_spark_session(db_connect_conf) #
            session.run(self.pipeline_name, node_names=[self.node_name])

# Kedro settings required to run your pipeline
env = "{{ env }}"
pipeline_name = "{{ pipeline_name }}"
project_path = Path.cwd()
package_name = "{{ package_name }}"

# Default settings applied to all tasks
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

# Using a DAG context manager, you don't have to specify the dag property of each task
with DAG(
    "{{ dag_name | safe | slugify }}",
    start_date=datetime(2019, 1, 1),
    max_active_runs=3,
    schedule_interval=timedelta(minutes=30),  # https://airflow.apache.org/docs/stable/scheduler.html#dag-runs
    default_args=default_args,
    catchup=False # enable if you don't want historical dag runs to run
) as dag:

    tasks = {}
    {%- for node in pipeline.nodes %}
    tasks["{{ node.name | safe | slugify }}"] = KedroOperator(
        task_id="{{ node.name | safe | slugify  }}",
        package_name=package_name,
        pipeline_name=pipeline_name,
        node_name="{{ node.name | safe }}",
        project_path=project_path,
        env=env,
    )
    {%- endfor -%}

    {%- for parent_node, child_nodes in dependencies.items() -%}
    {%- for child in child_nodes %}
    tasks["{{ parent_node.name | safe | slugify }}"] >> tasks["{{ child.name | safe | slugify }}"]
    {%- endfor -%}
    {%- endfor -%}
